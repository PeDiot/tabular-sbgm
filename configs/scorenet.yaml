data: 
  numeric: [
    "limit_bal", 
    "age", 
    "pay_0", 
    "pay_2", 
    "pay_3", 
    "pay_4", 
    "pay_5", 
    "pay_6", 
    "bill_amt1", 
    "bill_amt2", 
    "bill_amt3", 
    "bill_amt4", 
    "bill_amt5", 
    "bill_amt6", 
    "pay_amt1",
    "pay_amt2", 
    "pay_amt3", 
    "pay_amt4", 
    "pay_amt5", 
    "pay_amt6"
  ]
  category: [
    "default_payment_next_month",
    "sex", 
    "education", 
    "marriage"
  ]
  target: default_payment_next_month 
  target_class: 1
  classification: true
  test_prop: .2
  batch_size: 64
  logit_transform: false
model: 
  input_layer: 
    [
      [Linear, 30, 512],     # number of input features
      [Dropout, 0.3], 
      [Tanh]
    ]
  hidden_layers: 
    [
      [Linear, 512, 512], 
      [Dropout, 0.2], 
      [Tanh], 
      [Linear, 512, 256], 
      [Dropout, 0.1], 
      [ReLU]
    ]
  output_layer:
    [[Linear, 256, 30]]       # number of input features
training: 
  algo: "dsm"                
  noise_std: .1                # dsm
  n_particles: 10              # ssm
  n_epochs: 5000
  snapshot_freq: 3750
  n_steps_min: 3750
  eval_freq: 375
  stop_threshold: .001
  n_steps_no_improvement: 2
  resume_training: false
optim:
  optimizer: 
    weight_decay: 0.000
    name: "Adam"
    lr: 0.001
    beta1: 0.9
  scheduler: 
    name: ExponentialLR
    gamma: .9
  use_scheduler: false
sampling: 
  n_steps: 1000
  step_lr: 0.00002
  n_batches: 3
  burn_in: .7                  # proportion of mcmc samples to burn 
backup: 
  save: true
  dir: backup/taiwan_payment/scorenet/dsm
  checkpoint: -1
device: cpu
mode: train