data: 
  numeric: [
    "limit_bal", 
    "age",
    "pay_0", 
    "pay_2", 
    "pay_3", 
    "pay_4", 
    "pay_5", 
    "pay_6", 
    "bill_amt1", 
    "bill_amt2", 
    "bill_amt3", 
    "bill_amt4", 
    "bill_amt5", 
    "bill_amt6", 
    "pay_amt1",
    "pay_amt2", 
    "pay_amt3", 
    "pay_amt4", 
    "pay_amt5", 
    "pay_amt6"
  ]
  category: [
    "default_payment_next_month",
    "sex", 
    "education", 
    "marriage"
  ]
  target: default_payment_next_month 
  target_class: 1
  classification: true
  test_prop: .2
  batch_size: 64
  logit_transform: false
model:                        # https://github.com/ermongroup/ncsn/blob/master/models/scorenet.py
  input_layer: 
    [
      [Linear, 30, 1024],     # number of input features
      # [Dropout, 0.3], 
      [ELU]
    ]
  hidden_layers: 
    [
      [Linear, 1024, 1024], 
      # [Dropout, 0.2], 
      [ELU], 
      [LayerNorm, 1024],
      [Linear, 1024, 512], 
      [LayerNorm, 512],
      # [Dropout, 0.1], 
      [ELU]
    ]
  output_layer:
    [
      [Linear, 512, 30],       # number of input features
      [LayerNorm, 30]      
    ]  
training:     
  algo: "dsm"                
  anneal_power: 2.0           # dsm
  sigma_begin: 1             
  sigma_end: 0.01         
  n_sigmas: 100   
  n_particles: 1              # ssm
  n_epochs: 200
  snapshot_freq: 1000
  n_steps_min: 1000
  eval_freq: 1000
  n_steps_no_improvement: 3
  resume_training: false
optim:
  optimizer: 
    weight_decay: 0.000
    name: "Adam"
    lr: 0.001
    beta1: 0.9
  scheduler: 
    name: ExponentialLR
    gamma: .9
  use_scheduler: false
sampling: 
  n_steps_each: 100
  step_lr: 0.00002
  n_batches: 3
  burn_in: .7                   # proportion of mcmc samples to burn 
  strategy: mean                # average or all or median
backup: 
  save: true
  dir: backup/taiwan_payment/annealscorenet/dsm2
  checkpoint: -1
device: cpu
mode: train